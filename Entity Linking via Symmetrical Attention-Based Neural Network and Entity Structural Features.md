# Entity Linking via Symmetrical Attention-Based Neural Network and Entity Structural Features

# 1.介绍

**实体链接**：给定一个文档d，包含一个mention集M={m1,m2,...,mL}，和一个知识库，其中包含实体集ε={e1,e2,...,eN}，实体链接的任务就是找出<mi,ej>这样的对，使每个mention能够匹配到知识库中的实体。

目前实体链接模型可以被分为两类：局部方法和全局方法。局部方法利用语义特征，为每个实体独立地选取候选实体，这种模型在语料库包含丰富的上下文信息时效果很好。全局方法利用文档特征来提取和表示mention特征，考虑到同一篇文档中的mention相互之间有所联系。

目前所有的模型都忽略了3个要点：

- 实体和mention之间相互关联，尤其是在语义特征上
- 实体和关系是知识库的基本单元，同等重要且有联系
- attention机制能过滤掉上下文中无用的信息，但这些信息在构建实体embedding时同样被忽略掉了

本文提出一种新的实体链接模型来解决上述问题，使用对称Bi-LSTM和attention机制和实体结构化特征，称为SA-ESF。SA-ESF包含3个步骤：

- **生成候选实体**：用4种策略生成候选实体，并能过滤噪声和提高准确度上限。
- **结合词向量和实体向量进行学习**：用skip-gram模型将符号特征映射成连续的低维向量。
- **用带attention机制的对称Bi-LSTM消除实体歧义**：用带注意力机制的对称Bi-LSTM生成每个mention的向量和其候选实体，再计算mention和其候选实体间的语义相似度，最后用不同的权重整合语义相似度和先验概率来得到正确的<m,e>对。

# 2.相关工作

传统实体链接系统可以分为两类：独立方法和集合方法。

- 独立方法

  > 这种方法中，上下文特征只被用于消除mention歧义，将任务转化为基于mention和实体相似度的候选实体排序。不同的手动设计的特征被用于计算相似度，例如名字字符串相似度、先验概率、上下文相似度等。无监督方法被用于给候选实体排序，而分类器则用监督式方法训练。这种方法忽略了同一篇文档中实体之间的联系。

- 集合方法

  > 这种方法考虑到同一篇文档中mention之间的联系，相应的实体之间也有联系。目前最主流的方法是将候选实体构建成一张无向图并利用不同的图最小算法来找出正确的实体。总体上这种方法效果比独立方法要好。

SA-ESF有以下几个特点：

- 只用LSTM不能准确地抓住特征，因此我们用带attention机制的Bi-LSTM来获取上下文特征，可以过滤掉低价值的特征同时表示位置特征。
- 结合实体ID特征、上下文特征和结构特征来表示实体embedding，能够捕获之间的关系。
- 用基于实体特征的attention机制来捕获mention特征，能够有效保留mention和实体之间的关系。

# 3.方法

实体链接系统可分为两步：候选实体生成和候选实体排行。具体来说：

1. 用两个字典（别名和频率）和一个知识图谱提取实体特征
2. 用训练好的对称Bi-LSTM生成mention和候选实体的embedding
3. 计算每个mention和其候选实体之间的相似度并排序，生成结果

## 3.1生成和过滤候选实体

本文用斯坦福NER工具来侦测mention，判断其是否需要消除歧义，再用本地知识库提取候选实体。由于每个mention有很多表现形式，因此不能直接用原始mention来找候选实体。为此，我们用以下4种策略来改善结果。

### 3.1.1策略1：消除噪声

一些mention会包含附加词、标点符号、前缀后缀等，需要把这些因素消除。



### 3.1.2策略2：频率字典

该方法用键值对来统一每个mention的形式。具体来说，实体和mention是从维基百科的首页上收集的，每一页都包含一些链接，指向其他的实体。每个包含链接的文本就能与其连接的实体相对应，同时也是该实体的一种形式。因此很容易就能获得一个包含若干 <mention,候选实体> 这样键值对的字典。

除此之外，链接文本还能用来统计每个键值对的频率，在计算先验概率时有用。

### 3.1.3策略3：维基百科功能页面

除了链接文本外，维基百科还包含实体标题、重定向页面（同义词）、消岐页面（多义性）能够帮助区分不确定的候选mention。

### 3.1.4策略4：别名字典

有些mention是对应实体的别名，很难从表现形式上推测。为此我们构建了一张别名字典，记录实体名和它们可能的别名，这些对应关系可以从维基百科每个实体的信息框中获得。

## 3.2联合特征embedding

这一步生成每个mention和实体的embedding，本文使用word2vec学习每个符号的向量。模型同时训练词向量和实体向量。具体来说，对每个输入句子，将其中任意或所有的链接文本替换为对应的实体名。



3个优势：

- 能更精准地描述mention和实体特征
- 保留部分词特征能同时训练mention和实体特征，并获得之间的联系，能够更全面地描述mention和实体特征
- mention和实体的向量表示在同一个向量空间中，使相似度计算更容易

## 3.3通过DNN的实体消歧

DNN在实体特征提取上的劣势：

- 忽略了重要的实体描述信息。
- 忽略了mention和候选实体上下文中的词序
- 忽略了实体的结构信息

本文SA-ESF的优势：

- 将对称Bi-LSTM集成到mention和候选实体特征的提取中
- 优化了对称Bi-LSTM的输入
- 用实体描述和知识图谱结构提取实体特征

模型包含3个基本单元：候选实体特征提取，mention特征提取，相似度计算。先提取出描述embedding和结构embedding并合成实体embedding，再用对称Bi-LSTM将词向量、实体embedding和实体ID embedding合成mention embedding，最后将mention和候选实体embedding输入两层隐含层，用sigmod函数生成相似度并输出。

### 3.3.1带attention机制的对称Bi-LSTM

[LSTM和Bi-LSTM](https://blog.csdn.net/lgy54321/article/details/99936770)

由于LSTM只能提取句子的单向特征，因此我们用Bi-LSTM来提取双向特征。输出h(t)由正向和反向两个LSTM的输出拼接而成，单个输出为k维，拼接后h(t)为2k维。再用一个激活函数将h(t)转化为k维：

![image-20201205103600153](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20201205103600153.png)

由于标准Bi-LSTM很难判断输入序列是否有意义，会导致向量噪声和可靠度下降，因此用attention机制来抓住输入序列的关键点。

设H是一个由Bi-LSTM的n个隐含状态构成的向量，每个隐含状态向量为d维，H为d×n维。用以下公式生成带权隐含层表示：

![image-20201205104909705](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20201205104909705.png)

其中va表示attention向量（mention或实体embedding），en是一个向量，α表示attention的权值向量。Wh和Wa是需要学习的矩阵。将输出送到max-pooling过程，生成固定长度的最终表示。

### 3.3.2实体特征提取

#### 实体结构embedding

在知识库中，三元组是基本单元，描述为(eh,et,r)，eh和et表示头和尾实体，r表示关系。我们用CombinE模型来提取结构信息。CombinE用加结合和减结合来描述实体的内在和外在特征。加结合能表示实体对的共性，减结合能表示各自的异性。最终将这两种结合向量相结合，得到结构向量**es**。

#### 实体ID embedding

先将语料库中的mention用维基的实体ID替代，再用skip-grim方法生成实体ID embedding **ei**，能够描述整体的实体特征。

#### 实体描述embedding

用Bi-LSTM来提取实体描述特征。框架从下到上包含4个部分：embedding层，Bi-LSTM层，attention机制，max-pooling。步骤：

1. 从维基页面提取出实体描述
2. 将embedding序列作为Bi-LSTM输入，提取出每个实体的潜在特征
3. attention机制判断每个词的重要性
4. max-pooling用于改善性能并获取实体描述embedding **ed**

---

最终将这几种embedding相结合得到 e = [es,ei,ed]。

### 3.3.3mention特征提取

结构与实体特征提取模型类似，不同的是在embedding层，我们结合词向量、实体embedding和mention-word embedding来表示输入特征。

#### 词向量

用3.2节的模型构建词向量**x**

#### 实体embedding

用3.3.2节的模型生成实体embedding **e**

#### mention-word embedding

每个mention由s个词组成序列，mention-word embedding **mv**直接用这s个词的词向量的平均来表示。

---

最终输入表示为[x,e,mv]。之后用Bi-LSTM来提取每个词的隐含特征，再用attention机制提取上下文的重要单元（用mention embedding调整上下文权重，用实体embedding计算上下文权重），最后用max-pooling选择用来表示mention embedding **m**的重要特征。

### 3.3.4相似度计算

将mention和候选实体的embedding输入两个隐含层和一个sigmoid函数来计算相似度，最后计算候选实体排序。

相似度计算结果为sim(m, e)，由频率字典得到先验概率p(e|m)，最终排行分数为：
$$
r(m,e) = αp(e|m)+βsim(m,e)
$$
α和β位权值参数，和为1。

# 4.实验结果

**候选实体生成策略分析**：

- 所有策略都能改善召回率，策略2（频率字典）增幅最高
- 相较于新闻，推特这种短句更难识别
- 原始策略下中文数据集效果更差，英文中文文本需要分词，这会带来大量噪声

**参数分析**：

- 先验概率比相似度更重要，即α比β更大
- 新闻数据集中β比其他数据集更小，说明新闻中的mention更容易识别