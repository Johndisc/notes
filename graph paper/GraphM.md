# GraphM: An Efficient Storage System for High Throughput of Concurrent Graph Processing

## 1.介绍

大量的并发迭代图处理工作是在一个相同的云平台上执行的，但目前的工作主要集中在优化单个图处理任务。

为了实现高效的并发迭代图处理，有两个挑战需要解决：

1. 目前在同一个底层图上运行的并发任务没有考虑数据访问的相似性，造成了不必要的数据访问开销。

   > 并发迭代图处理任务常常重复遍历相同的图结构。然而由于图储存引擎和图处理框架的高度耦合，导致多份共享图数据的副本都被储存在cache或内存中，并且各自单独被访问，导致低效的数据访问和存储资源使用。

2. 各种图处理系统都和他们自己的储存引擎高度耦合。

   > 希望能将图处理系统与图储存系统解耦，共享一个优化的图处理系统。这样能将优化的储存系统关于处理引擎相整合来来实现迭代图处理应用的并发和高效执行，同时减小给用户的负担。

为了解决上述问题，提出储存系统GraphM。它是一个轻量的运行时系统，能在任何图处理系统上使用并支持迭代图处理任务的并发执行。提出共享同步机制来利用并发任务数据的相似性，能把多个特定任务共享的图结构数据解耦出来，只保存各个任务独特的数据，然后规定并发任务的遍历路径。

![image-20211118144113826](D:\notes\assets\GraphM\image-20211118144113826.png)

## 2.GRAPHM概述

### 2.1系统架构

一般来说，一个迭代图处理任务所需要的数据包括图结构数据（例如G=(V,E,W)），任务特有的数据（例如PageRank的分数），标记为S。GraphM能将G标称共享的，包含3和主要的部分：图预处理器，图共享控制器，同步管理器。

#### 图预处理器

不同的图处理系统需要的图格式不同，因此处理之前先要用用户定义的convert()函数进格式转换。然后，将图划分为几个部分用于并行处理，并根据该任务的一般遍历顺序进一步被划分和编号，储存在LLC(last level cache)中，并用一个chunk_table数组记录每个图划分的关键信息。

#### 图共享控制器

这个模块用来指定加载顺序并加载共享的图结构，仅被设计为一个简单的API，挂入现存的图处理系统。表示为
$$
Pij←Sharing(G,Load())
$$
G表示加载的图，Load()为图处理系统原本的加载操作，Pij表示第j个任务共享的一个加载的图结构部分Pi。

#### 同步管理器

不同的任务独自访问图，可能会跳过非活跃节点，计算复杂度也各不同。因此共享图数据会不规律地流入LLC，导致不必要的访存开销。因此，我们采用细粒度同步方式来利用任务之间的时间相似度。

具体步骤：每个任务需要在每轮迭代前说明每个图划分的计算负荷，然后据此分配计算资源，以此来同步图遍历。这样每个图划分只需要被载入到LLC一次，并在每次迭代中并发重复使用。

#### 编程API

为了调用GraphM，只需在图处理系统中添加我们的API。

- `init()`            			        	通过预处理图来初始化GraphM
- `sharing()`                              插入图处理系统中，替换掉原本的数据加载操作，对于不同的系统，函数参数不一样
- `GetActiveVertices()`         在每次迭代前获取活节点

### 2.2图预处理

CPU使用率和cache局部性会被图划分大小影响，表示为Sc。设置太大时会增加数据访问开销，因为当只有图划分的一部分能被加载进LLC时，剩下部分的加载会替换掉这部分，不同的并发任务会重复替换LLC，导致开销增大；设置太小会导致并发任务之间频繁的同步，因为只有当并发任务完成了对这个图划分的处理它们才能处理下一个。

Sc通过以下公式决定：
$$
S_{c}\times N+\frac{S_{c}\times N}{S_{G}}\times \left | V \right |\times U_{v}+r\leq C_{LLC}
$$
$N$示CPU核数，$C_{LLC}$表示LLC的大小，$S_G$表示图数据的大小，$|V|$表示图的节点数，$Uv$表示每个节点的数据大小，$r$表示LLC的保留空间，$S_c$被设定为满足该不等式的最大值。

前者表示任务并行处理所需要的LLC大小（进程数=CPU核数），后者表示用来储存任务特定数据的LLC大小。通过这种设置，相同的图划分只会被加载到LLC中一次并被反复使用，只有任务特定的数据需要被其他任务替换。

在预处理阶段，会先遍历一遍，把每个部分按它们进入LLC的顺序进行标记。标记储存在表chunk_table中，每行是一个键值对，key是每图划分起始点的ID（表示为v），value是其图划分内出度。

### 2.3图结构内存共享

#### 2.3.1共享图结构

一般来说，图结构被图划分为多个部分进行并行处理。每个任务处理的活跃部分不一样，但可以通过跟踪图处理系统每次迭代时访问的活跃部分获得。一个全局表单用于储存该信息。每一行是一个链表，储存该图部分所对应的任务进程pid。于是，设计了sharing()函数来让任务之间共享图结构。

![image-20211118095030433](D:\notes\assets\GraphM\image-20211118095030433.png)

#### 2.3.2一致性

当一个共享图结构需要修改时，用户要用我们的API来处理涉及到的图。首先，将需要修改的图图划分复制到另一图划分共享内存，再将修改应用到副本上，并将任务中图划分的虚地址映射修改到副本上。

图更新则需要在修改后应用于后来的其他任务，因此后来的任务会访问更新过的图，进行中的则会访问图副本。

![image-20211229181349485](D:\notes\assets\GraphM\image-20211229181349485.png)

### 2.4细粒度同步

#### 2.4.1获取并发任务之间的相似性

细粒度同步机制需要获取共享图中每次迭代中能被并发处理的图划分。首先监视当前迭代过程中每个任务需要处理的图划分，有些图划分会跳过没用的部分，例如SSSP每次只遍历一部分，PageRank必须全部遍历。一般只有在当前迭代中邻居有更改的节点才会在下一次迭代中更新。于是用一个bitmap来标记每个任务的活跃节点。

#### 2.4.2遍历的细粒度同步

每个任务会被分配不均等的计算资源。一般来说，每个任务的计算负载不仅由边的数量决定，还取决于边处理函数的计算复杂度$T (F_j )$，平均边数据访问时间为$T(E)$。因此每个同步过程分为两步，即分析和同步。

在分析步骤中，首先获取新提交任务的执行时间$T_{ij}$，表示为
$$
\begin{aligned}
&T\left(F_{j}\right) \times \sum_{k \in C^{i}} \sum_{v \in V_{k} \cap A_{j}} N_{k}^{+}(v)+ \\&T(E) \times \sum_{k \in C^{i}} \sum_{v \in V_{k}} N_{k}^{+}(v)=T_{j}^{i}
\end{aligned}
$$
$C_i$是$P_i$部分中的图划分集合，$V_k$是第$K$图划分中的点集，$A_j$是当前遍历中活跃节点，$N_k^+(v)$是第k图划分点v的出度

在同步步骤中，第j个任务处理第k图划分的负载为
$$
L_{j}^{k}=T\left(F_{j}\right) \times \sum_{v \in V_{k} \cap A_{j}} N_{k}^{+}(v)
$$
为了实现细粒度同步和更好的局部性，处理相同图部分的任务需要移到同一个CPU上执行。

## 3.OOO核的调度策略

乱序核中图图划分的加载顺序可能会导致并发任务数据访问的不充分利用，原因有两点：

1. 一些任务当前迭代处理的图图划分可能不多，但下一次迭代就会激活更多节点，例如BFS核SSSP
2. 激活的图图划分可能会在当前迭代被其他任务访问，因此一个图划分可能会被重复加载进内存，例如PageRank和WCC

更改加载顺序不会影响最终结果，核心思想是加载任务所需的最少的激活图划分。

![image-20211118145914651](D:\notes\assets\GraphM\image-20211118145914651.png)

为了实现，每个图划分会被设定一个优先级，高优先级的先加载。两个设置优先级的规则：

1. 如果处理图划分的任务的激活图划分更少，则该图划分的优先级更高
2. 处理该图划分的任务更多，优先级更高

优先级公式：
$$
\operatorname{Pri}\left(P^{i}\right)=M A X_{j \in J^{i}} \frac{1}{N_{j}(P)} \times N\left(J^{i}\right)
$$
$Pri(P^i)$表示第Pi个图划分的优先级，$N_j(P)$表示第j个任务的活跃图划分，$N(J^i)$表示任务数。

## 4.实验

### 4.1实验设置

5个数据集，4个图计算算法(WCC，PageRank，SSSP，BFS)。轮流提交这4种算法，直到数量达到预设值，算法参数随机。将GraphM与GridGraph 相结合，称为GridGraph-M，与GridGraph-S和GridGraph-C比较，这两个分别是线性任务处理和并行任务处理。

### 4.2预处理成本

包括创建划分表和标记图划分，当图大小小于内存大小时，平均多4%，大于内存大小时，平均多16.1%。

### 4.3整体性能

![image-20211118163827598](D:\notes\assets\GraphM\image-20211118163827598.png)

![image-20211118164555789](D:\notes\assets\GraphM\image-20211118164555789.png)

![image-20211118165434620](D:\notes\assets\GraphM\image-20211118165434620.png)

- 图9，16个并发任务的整体运行时间，相较于S和C分别平均提升2.6倍和1.73倍
- 图10，内存访问时间，在UK-union上相较于S和C分别减少了11.48倍和13.06倍。
- 图11，内存占用情况，在UK-union上是S的8.2倍，但是C的71%。
- 图12，整体IO负载，在UK-union上相较于S和C分别减少了9.2倍和10.1倍。
- 图13，LLC利用率，在UK-union上M，S，C的缺失率分别为15.69%，45.3%和43.3%。
- 图14，LLC数据交换，S是C的65%，M是S的55%。
- 图15，模拟真实情况的任务提交，M相较于S和C分别提升1.5–7.1倍和1.48–9.8倍。

### 4.4调度策略性能

在Clueweb12上有调度策略的时间是没有的72.5%

### 4.5和其他系统结合

![image-20211118165745848](D:\notes\assets\GraphM\image-20211118165745848.png)

