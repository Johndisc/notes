# Gengar : An RDMA-based Distributed Hybrid Memory Pool

### 1.介绍

字节寻址的**非易失性内存（NVM）**技术能提供高内存密度，低成本和几乎为0的功耗，但和DRAM相比读写表现不行，因此常常与DRAM组成混合内存系统。此前的研究主要集中在单个机器环境下，很少用在**分布式共享混合内存（DSHM）**系统上。

**远程内存直接访问技术（RDMA）**能够提供低延迟的远程内存访问。目前在DSHM上管理NVM和RDMA 还有一些挑战：

1. DRAM缓存方案对DSHM无用，因为RDMA不走操作系统。传统基于OS的内存监控方案没用。
2. 大多数RDMA操作是异步的，但这种特点没有被用来降低RDMA的写延迟。如何充分利用RDMA的异步模式并且保证数据的持久性和一致性还是个问题。

本文提出Gengar，一个启用RDMA的DSHM池。Gengar利用RDMA的读写来实现低延迟的远程内存访问，也提供了一系列DSHM上RDMA的编程API。Gengar通过几项创新设计来解决上述问题：

1. 提出高效的DRAM缓存方案来加速远程NVM访问。通过利用客户端的RDMA读写语义设计了一个轻量的内存访问监控机制，来识别远程内存服务器上频繁读写的数据（热数据）。然后将热数据缓存到DRAM缓冲区中来加速远程NVM访问。我们实现了分布式DRAM缓存作为写穿cache来仅仅加速NVM读操作，同时RDMA写操作直接在NVM上进行来保证数据一致性。
2. 通过重新设计RDMA的写原语提出了一种新型的RDMA写模式。利用一种代理机制来等待RDMA Work Completion(WC)时间，允许应用执行与数据传输同时进行。通过这种方式能隐藏RDMA写操作的网络RTT。
3. 提供简单的API来保证数据的一致性。利用租约分配机制和轻量写操作锁来保证元数据的一致性。

### 2.背景介绍

#### A. 混合型内存和RDMA

 Intel  Optane  DCPMM的出现能与DRAM相结合来构建大容量和节能的混合型内存系统。DCPMM有两种模式：

1.  App Direct模式。DRAM作为主存，DCPMM作为储存，通过特殊API为应用提供直接访问的字节寻址储存。
2. 内存模式。DCPMM作为大容量，低性能的主存，DRAM作为OS不可见的cache来弥补DRAM和NVM之间的性能差距。

虽然DCPMM能大大提升HPC和数据中心的性能和功耗，但其本身性能比DRAM差不少。目前的很多研究都注重于单节点上的融合，DSHM上不太行。

RDMA能够让应用不通过OS，直接访问远程服务器，消除了缓冲区之间的复制。两种RDMA：**单边RDMA**用读写动词，**双边RDMA**用收发动词。单边通过避免远程服务器CPU的参与，比双边有更低的延迟和更高的带宽。

RDMA分为3种模式：可靠连接（RC），不可靠连接（UC），不可靠数据报（UD）。单边RDMA基于两个节点的队列，称为Queue Pair(QP)。QP包括一系列工作队列：发送队列（SQ），接收队列（RQ），完成队列（CQ）。进行一个RDMA操作时，客户端将一个工作请求（WR）传到SQ中，当一个WR完成后，一个工作完成（WC）事件被传到CQ中。当客户端接收到WC后，该RDMA操作完成。

#### B. DSHM的优化机遇

目前远程访问NVM的高延迟是DSHM的主要问题。有一下几点优化方案：

1. RDMA的异步特征能用来优化写操作。从应用的角度来看，RDMA的写延迟还是很高，因为一个WR必须等待远程NIC传回WC事件。
2. DRAM缓存方案在单个机器上的混合内存系统中很有效，应该将其扩展到DSHM上。RDMA的读操作同城是同步模式，延迟主要取决于网络RTT和远程NVM读延迟。因此需要利用分布式DRAM缓冲区来加速NVM访问。

#### C. 设计挑战

1. 传统在单节点上对热数据的页面迁移/缓存技术不适用于DSHM。以前的DRAM缓存技术依赖于OS层的机制，但RDMA不涉及CPU或OS。
2. RDMA的写操作是主要瓶颈。RDMA写操作的异步性会导致数据不一致问题。
3. 多个应用共享分布式混合内存时需要保证数据一致性。

### 3.设计与实现

#### A. 架构

客户端运行应用，访问远程节点的内存。每个客户端在本地维持一个远程服务器列表，每个服务器将其DRAM/NVM 可用容量与RDMA的ACK一起报告给客户端。

Gengar将所有服务器的NVM资源作为一个共享内存池来管理，用一小部分DRAM作为分布式缓冲区来加速NVM访问。开始，客户端从共享内存池中给应用分配NVM，当一个对象被认定为热数据，客户端就会从内存池中请求DRAM来缓存。DRAM缓存能存在于任何一个节点，因此热对象增加也没关系。回收热对象时同时回收DRAM缓存和共享NVM池。

传统文件系统提供大量接口来使用远程内存，但增加了软件层的延迟。Gengar选择一种基于对象的DSM设计来直接访问远程内存，提供了非常简单的API。

共享NVM的分配和释放是通过请求/相应的通讯模型实现的。

- 客户端需要分配更多远程资源时，向服务器发送一个内存分配请求。成功分配后，服务器返回给客户端一个全局内存地址，虚到实的地址映射储存在一个哈希表（AMT）中，每个节点都在本地保存一份。当NVM数据被缓存到DRAM缓冲区中时，地址映射表中也会维持一个NVM到DRAM的映射表。

- 客户端应用的数据对象需要释放时，想服务器发送一个内存释放请求。如果对象缓存在DRAM缓冲区中，服务器就会同时释放DRAM和NVM的内存，对应的虚实地址映射也会从表中删除。

为了保证数据的持续性，DRAM缓存采用写穿策略。如果数据存在于DRAM缓存中，它会被同时写到DRAM缓存和NVM，否则只写NVM。这样能降低RDMA读延迟，且我们的写优化方案能消除其带来的写延迟。

#### B. DRAM缓存管理

Gengar在客户端利用RDMA的读操作来识别热对象。为地址映射表中的每个对象添加一个读计数器，每次读时计数加一，到达阈值时认定为热对象。每个客户端独自决定热对象，再将计数发送到服务器来全局选择热对象。

热数据的阈值时根据DRAM的使用情况动态调整的。DRAM资源充足时阈值降低，缓冲区满时淘汰部分冷对象来缓存热对象。Gengar在缓存新对象的好处和对象替换的开销之间进行权衡，只在有收益时进行缓存替换。

Gengar采用LRU淘汰算法，服务器淘汰时更新本地的地址映射表，并通知对应的客户端更新。

#### C. 一个优化RDMA写操作的代理机制

一般来说可靠连接的RDMA写操作包含5步：

1. 客户端应用注册一个RDMA操作的MR
2. 客户端将WR添加到发送队列SQ，并等待WC
3. RDMA处理WR并将数据传输给服务器
4. 创建WC并放到完成队列CQ
5. 应用继续运行并注销MR

网络时延RTT主要成剩余应用等待WC的过程。本文提出一种新型的RDMA写模式：一旦客户端NIC收到了WR就返回一个WC，使应用继续执行。同时RDMA通过类似于UC/UD模式来执行（①②③④）。

![image-20210820184145092](C:\Users\58253\AppData\Roaming\Typora\typora-user-images\image-20210820184145092.png)

MR多路复用技术能减少MR注册和注销的开销，但会带来多的内存复制开销。因此该技术只在传输小型对象时高效，随着对象大小增加，开销也会增加。因此在Gengar中只对小于1MB的对象采用该技术。对大型对象先用自己的内存注册，并在发送到远程服务器之前将其设置为只读。

对于我们的写策略，当发生两个WR写同一处内存地址时，会被RDMA通讯原语阻止，并造成远程访问错误。为了解决这个问题，Gengar在地址映射表上加了一个写tag。写操作时，当通过地址映射表获取远程地址时设置写tag，后面相同地址的写操作就被阻塞了。写操作完成时再将tag重置。

#### D. 并发性和一致性

仅仅用Gengar实现远程内存访问并发控制的代价是高昂的，因此应该由应用程序自己来实现并发控制，例如加锁。但Gengar也应该提供并发控制的API，并且在访问冲突时同时更新服务器和客户端的地址映射表，如果更新不及时则会造成数据不一致的问题。

1. 数据一致性

   > 保证数据一致性的简单办法就是广播，但数据同步时所有访问都会阻塞于网络延迟。Gengar利用一个服务端的租赁政策来减轻负担。Gengar用一个时间戳设置每个对象的租赁期限时间，每次释放或缓存对象时，客户端需要等待当前租赁的期限。当得到租赁时，可以直接进行RDMA操作，而不用频繁地在多个客户端之间同步数据。
   >
   > 租赁时间对DRAM的效率影响很大，我们根据对象的访问频率用爬山算法来动态调整。对NVM中的热对象设置短租赁使其更早缓存进DRAM，对DRAM中的热对象设置长租赁来避免过多重分配。

2. 并发控制

   > Gengar提供应用级API（set_lock）来支持并发控制。对于申请/释放/缓存/淘汰/读写操作，Gengar用一个锁和3个RTT来完成。第一个RTT，客户端向服务器请求锁，顺便更新对象元数据。第二个RTT，客户端用单边RDMA读写来传输，服务器收到后，客户端收到WC。第三个RTT，应用释放锁。

#### E. 一个支持老应用的文件系统接口

很多场景需要在不修改源码的条件下将老应用迁移至DSHM池。我们将Gengar与FUSE相整合来实现一个简单的分布式文件系统。

当应用访问DSHM池的数据时，用传统的文件系统接口，读写请求由FUSE_module发送到FUSE_daemon，由它来管理分布式文件系统。对象的元数据储存在客户端本地，数据储存在内存池中。在FUSE_daemon中，应用级的文件读写被转化为RDMA读写操作。FUSE_daemon收到数据请求后，首先访问本地地址映射表，然后通过RDMA接口访问远程内存池。
